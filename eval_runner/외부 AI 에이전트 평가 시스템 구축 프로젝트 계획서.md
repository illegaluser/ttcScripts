# 외부 AI 에이전트 평가 시스템 구축 프로젝트 계획서

**작성 일자**: 2026. 01. 20
**문서 성격**: 프로젝트 계획서
**프로젝트 명**: External AI Agent Evaluation System 구축
**적용 기술**: Promptfoo, DeepEval, Jenkins, Ollama, Langfuse, Pytest
**핵심 목표**: “평가 기준”을 **반복 실행 가능한 운영 체계**로 구현하여, 외부 AI 에이전트의 도입/재납품/운영 판단을 정량 근거로 지원한다.

---

## 1. 프로젝트 개요

### 1.1 시스템 구축 목적

본 시스템은 DSCORE-TTC 인프라를 기반으로, 외부(External)에서 개발되어 납품되거나 도입 예정인 AI 에이전트의 품질을 객관적인 정량 지표로 검증하기 위해 구축된다.
평가 결과는 도입 승인, 재납품 요청, 운영 중 품질 모니터링의 근거로 활용된다.

* **평가 주체(Evaluator)**: DSCORE-TTC 인프라(Jenkins, Eval-Runner 실행 환경)
* **평가 대상(Target)**: 네트워크로 연결 가능한 모든 외부 AI(사내망 챗봇, 외부 SaaS, 협력사 납품 봇 등)
* **평가 방식**: Black-box Testing(내부 코드를 보지 않고 입력/출력만으로 평가)

### 1.2 구축 배경 및 필요성

외부 AI 에이전트는 공급자별 구현 방식, 품질 관리 기준, 운영 조건이 상이하다.
도입 조직 관점에서는 코드의 우수성보다 **운영 환경에서의 결과 품질**과 **재현 가능한 검증 근거**가 중요하다.
본 프로젝트는 다음 문제를 해결한다.

* 검증 결과가 담당자 경험과 주관에 의존하여 재현성이 낮다.
* 업데이트/재납품 시 동일 기준을 반복 적용하기 어렵다.
* 보안 위반(PII 노출 등)과 같은 치명 리스크를 사후에 발견한다.
* 정량 근거가 부족하여 공급자와의 조치 협의가 비효율적이다.

---

## 2. 범위 및 적용 대상

### 2.1 평가 대상 분류

| 유형                         | 정의                   | 검증 핵심                        |
| -------------------------- | -------------------- | ---------------------------- |
| **Type A: RAG(검색 기반)**     | 문서를 검색하여 답변하는 봇      | 환각(Hallucination) 여부, 검색 정확도 |
| **Type B: Agent(자율 에이전트)** | API를 호출하여 과업을 수행하는 봇 | 과업 성공(Task Completion) 여부    |
| **Type C: Chatbot(대화형)**   | 일반적인 대화 및 상담 봇       | 보안 위반(PII), 동문서답 여부          |

### 2.2 구현범위

* 네트워크로 접근 가능한 외부 AI의 입력/출력 품질 평가 체계 구축
* Fail-Fast(보안/형식) 및 심층 평가(의미 기반) 지표 산출
* Jenkins 기반 반복 실행(Job) 및 표준 리포트 제공
* Golden Dataset 기반 케이스 관리 및 운영 가이드 제공
* 외부 AI 내부 코드 품질/설계(White-box) 진단은 포함하지 않는다.
* 외부 AI의 기능 개선(프롬프트/검색 튜닝 등)은 “평가 결과 기반 피드백”까지로 제한한다.
* 비용(Token Cost) 최적화는 1차 목표에서 제외하며, 필요 시 고도화 과제로 분리한다.

---

## 3. 운영 환경 및 전제(Immutable)

본 시스템은 DSCORE-TTC 자원을 공유하므로 아래 스펙을 기준으로 동작한다.

| 항목            | 고정 스펙                     | 기술적 배경                                          |
| ------------- | ------------------------- | ----------------------------------------------- |
| **Host**      | MacBook Pro M1 Max / 64GB | Unified Memory 기반으로 30B Judge 및 DevOps 스택 동시 구동 |
| **Runtime**   | Docker Desktop            | 단일 노드 운영, 구성 단순화                                |
| **Network**   | `devops-net`              | 컨테이너 간 격리 네트워크                                  |
| **Volume**    | `/var/knowledges`         | 영구 저장소(평가 데이터/리포트)                              |
| **Judge LLM** | Ollama(Native App)        | 호스트 프로세스로 구동하여 GPU 가속 최적화                       |

---

## 4. 프로젝트 목표 및 성공 기준

### 4.1 목표

1. 외부 AI를 대상으로 동일 절차로 평가를 반복 실행할 수 있는 체계를 구축한다.
2. 7개 핵심 지표를 통해 도입/재납품/운영 판단의 정량 근거를 확보한다.
3. Fail-Fast로 치명 리스크(보안 위반, 형식 파손)를 심층 평가 이전에 차단한다.
4. 결과가 운영자 관점에서 해석 가능하고, 공급자 조치 요청에 활용 가능한 형태로 제공된다.

### 4.2 완료 정의

* Jenkins Job에서 대상 URL을 파라미터로 주입하여 평가를 실행할 수 있다.
* Golden Dataset 기준으로 케이스가 수행되고 PASS/FAIL이 산출된다.
* 평가 리포트(JUnit 등)와 Trace(Langfuse)가 보관되어 사후 감사가 가능하다.
* 실패 케이스에 대해 “무엇이, 어떤 규칙/점수로, 왜 실패했는지”를 설명할 수 있다.

---

## 5. 평가 지표 정의 및 선정 근거

### 5.1 초기 후보 지표(16종) 검토 배경

초기 지표 16종은 “외부 AI 운영 품질을 구성하는 리스크 영역”을 누락 없이 검토하기 위해 도출하였으며, 검토 범주는 다음을 포함한다.

* 보안/준법(PII, 기밀, 정책 위반)
* 연동 안정성(응답 구조/계약 준수)
* RAG 품질(검색 성공, 근거 기반성, 환각 억제)
* 대화 품질(질문 의도 충족)
* Agent 실행 성공(과업 성공 판정)
* 운영 품질(지연, 안정성)
* 안전성/윤리(유해 발언, 편향, 톤 등)

이후 프로젝트 운영 제약(로컬 Judge, 결정론적 검증, CI 반복 실행, 측정 신뢰도)을 적용하여 7개 핵심 지표로 수렴하였다.

### 5.2 선정 기준

* **객관성(Objectivity)**: 재현 가능한 방식으로 측정 가능한가?
* **독립성(Independence)**: 공급자 내부 구현에 의존하지 않는가?
* **현실성(Feasibility)**: 운영 환경에서 안정적으로 수행 가능한가?

### 5.3 16종 → 7종 결과

(사용자 제공 표와 동일한 판정/근거를 유지하되, 본 계획서에서는 “왜 운영 체계로서 채택/배제되는가” 관점에서 정리한다.)

* **선정(7)**: Policy Violation, Format Compliance, Faithfulness, Contextual Recall, Answer Relevancy, Task Completion, Latency
* **통합**: Hallucination Rate(→ Faithfulness), Tool Selection/Argument(→ Task Completion)
* **탈락**: Context Precision, Answer Correctness, Token Cost, Toxicity/Bias, Tone/Style, Conciseness

---

## 6. 확정 지표(7종) 및 측정 방법(수단 포함)

### 6.1 Phase 1: 정적 분석(Fail-Fast) — Promptfoo + Schema

Fail-Fast는 “치명 결함을 심층 평가 이전에 차단”하기 위한 운영 게이트이다.

| 지표명                   | 측정 목적           | 측정 수단                 | 판정 데이터                  | 실패 처리                            |
| --------------------- | --------------- | --------------------- | ----------------------- | -------------------------------- |
| **Policy Violation**  | PII/기밀/금칙 정보 탐지 | Promptfoo(Regex Rule) | raw_response 전체 문자열     | 즉시 FAIL. 리포트에 위반 패턴/근거 기록 후 중단   |
| **Format Compliance** | 응답 구조 계약 준수     | Python `jsonschema`   | raw_response JSON 파싱 결과 | 즉시 FAIL. 누락 필드/타입 불일치 근거 기록 후 중단 |

### 6.2 Phase 2: 심층 분석 — DeepEval + Local Judge(Ollama)

의미 기반 품질을 평가한다. Phase 1 통과 후 수행한다.

| 지표명                   | 측정 목적          | 측정 수단                  | 필요한 입력                                      | 실패 처리                          |
| --------------------- | -------------- | ---------------------- | ------------------------------------------- | ------------------------------ |
| **Faithfulness**      | 근거 기반성(환각 억제)  | DeepEval + Local Judge | 질문, 답변, retrieval_context, ground truth     | 점수/근거(reason) 기록. 개선 권고 포함     |
| **Contextual Recall** | 정답 근거 검색 성공    | DeepEval + Local Judge | ground truth, retrieval_context             | 미회수 근거를 명시. 검색 개선 권고 포함        |
| **Answer Relevancy**  | 동문서답 방지        | DeepEval + Local Judge | 질문, 답변                                      | 의도 불일치 근거 기록. 프롬프트/전처리 개선 권고   |
| **Task Completion**   | Agent 과업 성공 판정 | Criteria Check(규칙 기반)  | status_code, raw_response, success_criteria | 실패 조건을 근거로 기록. 성공 신호 계약 재정의 권고 |

### 6.3 Phase 3: 운영 분석 — Langfuse

| 지표명         | 측정 목적    | 측정 수단                      | 기준               | 운영 처리                  |
| ----------- | -------- | -------------------------- | ---------------- | ---------------------- |
| **Latency** | 응답 지연 관리 | Langfuse Trace(Time Delta) | 기본 경고: 5000ms 초과 | 경고 누적 시 원인 분석 및 개선 과제화 |

---

## 7. 프레임워크 및 솔루션 선정

본 프로젝트는 “평가 기준을 실제로 반복 실행 가능한 운영 체계”로 만들기 위해 아래 도구를 선정한다.

### 7.1 DeepEval 선정 사유

1. **단일 실행 단위 제공**
   RAG/Agent/Chatbot을 케이스 단위로 동일하게 실행·리포트할 수 있어 운영 표준화가 가능하다.

2. **CI 통합 용이성**
   pytest 기반으로 Jenkins에서 표준 방식으로 실행되며, 결과를 JUnit 리포트로 관리할 수 있다.
   이는 운영자가 “빌드 번호 기준으로 이력”을 추적하는 데 필수이다.

3. **로컬 Judge 결합 가능**
   외부 API 기반 Judge는 데이터 유출, 비용, 통제의 문제가 있다.
   로컬(Ollama) Judge 결합은 평가 데이터의 통제, 반복 실행, 버전 고정에 유리하다.

### 7.2 Promptfoo 선정 사유

1. **정책 위반의 결정론적 차단**
   보안 위반은 점수 기반이 아니라 “즉시 차단”되어야 한다.
   Promptfoo의 정규식 규칙은 동일 입력에 대해 동일 판정을 제공하여 증적성이 높다.

2. **Fail-Fast로 비용/시간 절감**
   심층 평가는 Judge 호출이 포함되므로 비용과 시간이 크다.
   Fail-Fast는 불필요한 LLM 호출을 사전에 차단하여 운영 효율을 확보한다.

### 7.3 Ragas 제외 사유

1. **단일 이미지 운영에서의 충돌 리스크**
   특정 의존성 생태계에 대한 강한 종속성은 운영 환경 재현성을 저해한다.
   본 프로젝트는 “고정 버전 기반 반복 실행”이 핵심이므로 리스크를 우선 회피한다.

2. **운영 안정성 우선**
   지표 측정 자체보다 “항상 실행되는 체계”가 프로젝트 목표다.
   런타임 에러 가능성이 잦으면 평가 체계는 운영에서 탈락한다.

3. **기능 중복 대비 비용 과다**
   DeepEval의 커버리지가 확대된 현 시점에서 Ragas 추가 도입은 운영 복잡도만 증가시킬 가능성이 높다.

---

## 8. 프로젝트 추진 계획

### 8.1 추진 단계 및 마일스톤

* **M1: 기준 확정(01/15~1/20)**
  지표(7종) 정의, FAIL 조건, 임계치, 리포트 포맷 확정
* **M2: 실행 체계 구축(1/21~2/2)**
  Jenkins Job 구성, 파라미터 주입, 실행/보관 체계 마련
* **M3: 데이터셋 운영 체계 구축(1/26~2/2)**
  Golden Dataset 작성 규칙/검토 절차, 샘플셋 구축
* **M4: 검증 및 시범 운영((2/3~2/8)**
  1~2개 실제 외부 AI를 대상으로 평가 수행, 결과 해석 및 개선 루프 점검
* **M5: 기술혁신단장 시연(2/11)**

### 8.2 역할 및 책임(RACI 개요)

* **프로젝트 오너(PO/TPM)**: 범위/기준 승인, 대외 커뮤니케이션(공급자)
* **운영자(Ops)**: 케이스 작성/실행/리포트 해석, 재실행 관리
* **개발 지원(DevOps/QA)**: Jenkins/환경/스크립트 유지보수, 규칙/스키마 반영

### 8.3 변경관리(원칙)

* Golden Dataset, 정책 룰, 스키마 변경은 “평가 기준 변경”에 해당한다.
* 기준 변경은 이력 관리 및 승인 절차(추후 확정)를 통해 반영한다.

---

## 9. 산출물(Deliverables)

1. 프로젝트 계획서(본 문서)
2. 평가 지표 정의서(지표별 측정 수단/근거/임계치/실패 조치)
3. Golden Dataset 운영 가이드(작성 규칙, 검토 기준, 샘플셋)
4. Jenkins Job 정의 및 실행 가이드(파라미터, 결과 확인, 보관)
5. 리포트 템플릿 및 증적 체계(Langfuse 포함)

---

## 10. 리스크 및 대응(운영 관점) — 상세화

아래 대응 방향은 “원칙 수준의 문구”가 아니라, 실제 운영에서 적용 가능한 구체 조치 단위로 정의한다.

| 리스크            | 내용                             | 대응 방향(상세)                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| -------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Judge 변동성**  | 로컬 Judge 기반 점수 편차 가능           | 1) **Judge 고정 정책**: 모델명/버전/파라미터(temperature 등)를 문서로 고정하고, 변경은 변경관리 대상으로 취급한다. 2) **임계치 보수적 설정**: 초기에는 Fail을 과도하게 만들기보다, 명백한 결함만 잡히도록 임계치를 설정하고 운영 데이터로 보정한다. 3) **샘플셋 캘리브레이션**: Golden Dataset 중 일부를 “고정 샘플셋”으로 지정하여, Judge 업데이트나 환경 변화 시 점수 분포가 유지되는지 사전 점검한다. 4) **동일 조건 반복 실행**: 특정 케이스의 점수 편차가 의심될 경우, 동일 조건으로 N회 재실행하여 편차 범위를 확인하고, 편차가 큰 지표는 운영 게이트가 아닌 참고 지표로 한시 전환한다. 5) **근거 기록 중심 운영**: 점수 단일 값보다 reason(판정 근거)와 입력/출력 증적을 함께 제공하여 공급자 커뮤니케이션의 객관성을 확보한다. |
| **외부 API 불안정** | 대상 시스템 간헐 장애/지연                | 1) **타임아웃/재시도 정책 표준화**: 연결/응답 타임아웃과 재시도 횟수/간격을 프로젝트 기준으로 정의한다(예: 연결 10s, 응답 60s, 재시도 1~2회 등). 2) **오류 분류 체계**: “품질 FAIL”과 “시스템 오류(ConnError/5xx)”를 분리 기록한다. 오류는 공급자 품질 결함이 아니라 가용성 이슈로 별도 트랙킹한다. 3) **Latency와 실패의 분리**: 지연은 별도 지표로 누적 관측하고, 단발성 스파이크는 경고로 처리한다. 지속적 초과(P95 등)는 개선 과제로 전환한다. 4) **사전 헬스체크(선택)**: 평가 시작 전 간단한 ping/health endpoint 확인을 통해 “실행 가능 상태”를 확인하고, 불가 시 즉시 중단하여 불필요한 평가 수행을 방지한다.                                                           |
| **스키마 다양성**    | 공급자별 응답 구조 상이                  | 1) **응답 계약(Contract) 명문화**: 최소 필드(answer 등)와 타입을 표준 계약으로 정의하고 납품 조건에 포함한다. 2) **Format Compliance를 도입 승인 게이트로 운영**: 스키마 미준수는 의미 평가 이전에 중단한다. 3) **어댑터 확장 정책**: 예외적으로 공급자 구조를 수용해야 하는 경우, “어댑터 추가”를 별도 과제로 등록하고, 무분별한 분기 확장을 방지하기 위해 승인 절차를 둔다. 4) **스키마 버전 관리**: schema.json은 버전 태깅하여, 특정 기간의 평가 결과가 어떤 스키마 기준에서 산출되었는지 추적 가능하게 한다.                                                                                                                                 |
| **데이터셋 품질**    | Golden Dataset이 부실하면 평가 신뢰도 저하 | 1) **작성 규칙 표준화**: 케이스 ID 규칙, target_type, expected_output 작성 범위, ground truth 작성 방식 등을 명문화한다. 2) **커버리지 구조화**: 단순 사실(Level 1), 조건부 추론(Level 2), 환각 유도(Level 3)로 레벨을 나누고, 유형별 최소 개수를 기준으로 관리한다. 3) **검토 절차 도입**: 신규 케이스는 운영자 단독 작성이 아니라 검토자(업무 오너/규정 담당 등)의 확인을 거쳐 반영한다. 4) **정답 근거의 출처 관리**: RAG 케이스는 ground truth가 어떤 문서/조항에서 왔는지 추적 가능해야 하며, 문서가 변경될 경우 케이스 갱신이 필요하다. 5) **정기 리프레시**: 분기 또는 반기 단위로 “케이스 유효성 점검”을 수행하여, 실제 운영 정책 변경이 반영되지 않은 케이스를 정리한다.            |

---

## 부록 A. 용어 정의

* **Black-box Testing**: 내부 구현을 보지 않고 입력과 출력만으로 품질을 평가하는 방식
* **Fail-Fast**: 치명 결함이 확인되면 이후 단계를 수행하지 않고 즉시 중단하는 운영 원칙
* **Schema(스키마)**: 응답이 반드시 지켜야 하는 구조(필드/타입)에 대한 규칙
* **RAG**: 답변 생성 전에 문서 검색을 수행하여 근거를 확보하는 방식
* **Judge LLM**: 답변의 의미 품질을 평가(채점)하는 모델. 본 계획서는 로컬(Ollama) 사용을 전제로 한다.